{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "317e9c90-9d46-4133-babf-60e25cd5ee5f",
   "metadata": {},
   "source": [
    "## Intro to Neural Nets : Self-learning\n",
    "### Abstract\n",
    "The purpose of this notebook is to learn the basics of how neural networks work.\n",
    "I will be *somewhat* following Packt's *Machine Learning with Pytorch and Scikit-learn,\n",
    "starting with chapter 2's introduction to building simple discrete neurons, but will be \n",
    "integrating what I've previously learned from other sources. \n",
    "\n",
    "The resulting code is meant to be educational and illustrate concepts, not be \n",
    "performative; the final neural net won't be remotely optimized.  A later notebook will \n",
    "focus on implimenting a neural network in a more functional and practical manner using\n",
    "numpy's optimized data structures and matrix functions.\n",
    "\n",
    "*(additionally, I feel I need to review my linear algebra, so rather than leverage numpy \n",
    "I will instead be writing my own linear algebra functions in pure python)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f79954b-5e3a-4801-a981-8f4267e42763",
   "metadata": {},
   "source": [
    "### Part 1: Building a neuron \n",
    "At it's most basic, an artificial neuron consists of three main components: weights; a bias; and an activation function.\n",
    "- **weights:** Each neuron has one or more inputs. Each of these inputs is multiplied by a weight that determines how much and in what direction each of these inputs should affect the neuron's output. After the inputs have been weighed, they are then summed.\n",
    "- **bias :** After the sum of weighed inputs has been computed, an additional value (+ or -) that is added to that sum.  This value makes it easier or harder for this neuron to turn on, effectively biassing it towards one state or another.\n",
    "- **activation function:** Because the weights and biases are unbound, the sum of the biases and weighed inputs is also unbound in both directions -- this is unwanted.  The activation function is a final step that conditions the sum in such a manner that it is bound to be no less than 0 and possibly no more than 1 depending on the exact activation function used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e2cb6b-8714-435b-94d3-de9b1147b5f6",
   "metadata": {},
   "source": [
    "First we need to be able to instantiate a neuron with those 3 basic components.  Let's write a constructor that initializes it with random weights and biases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c763a26-37f0-4c40-b1ad-b177ebf67aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from random import random\n",
    "\n",
    "class Neuron():\n",
    "\n",
    "    def __init__(self, num_of_inputs: int, activation_function: str):\n",
    "        \"\"\"Instantiates a neuron with random weights and bias\"\"\"\n",
    "\n",
    "        # validate args:\n",
    "        if num_of_inputs < 1:\n",
    "            print(\"there must be more than 1 input to a neuron\")\n",
    "            return\n",
    "\n",
    "        self.num_of_inputs = num_of_inputs\n",
    "        self.weights = [random() for i in range(num_of_inputs)]\n",
    "        self.bias  = random()\n",
    "        if activation_function == \"RELU\":\n",
    "            self.activation_function = Neuron.RELU\n",
    "\n",
    "\n",
    "# For an activation function, let's start with a simple RELU.\n",
    "# Lets do this by extending the Neuron class with a class RELU method\n",
    "# NOTE: Extending a class via self-inheritance isn't something I'd typically do!\n",
    "\n",
    "class Neuron(Neuron):\n",
    "    \n",
    "    @classmethod\n",
    "    def RELU(self,sum_of_weighed_inputs_and_bias):\n",
    "        if sum_of_weighed_inputs_and_bias < 0:\n",
    "            return(0)\n",
    "        else: \n",
    "            return(sum_of_weighed_inputs_and_bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d4b0ed-ce10-40c4-907e-7d5f809abb39",
   "metadata": {},
   "source": [
    "Now we need to add a method to allow this neuron to take a vector of inputs and compute an output.  This involves multiplying the transposed weights vector by the inputs vector and then adding the bias.  So first we need functions to transpose and multiply matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76f21bed-ac1a-403d-a3fa-ecd8b898a3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_transpose(matrix):\n",
    "    # If a matrix has n columns and m rows, its transpose\n",
    "    # will have m columns and n rows, so let's build a \n",
    "    # zeros matrix with that shape:\n",
    "    cols = len(matrix)\n",
    "    rows = len(matrix[0])\n",
    "    mT = [[0 for _ in range(cols)] for _ in range(rows)]\n",
    "    \n",
    "    # use generator comprehension to populate zeros matrix\n",
    "    elements = (elem for row in matrix for elem in row)\n",
    "        \n",
    "    for col in range(cols):\n",
    "        for row in range(rows):\n",
    "            mT[row][col] = elements.__next__()\n",
    "\n",
    "    return(mT)\n",
    "\n",
    "\n",
    "def matrix_mult(A,B):\n",
    "    #Check that inner dimensions are equal:\n",
    "    assert len(A[0])==len(B)\n",
    "    \n",
    "    rows_A = len(A)\n",
    "    len_n = len(B)\n",
    "    cols_B = len(B[0])\n",
    "    \n",
    "    C = [[None for j in range(cols_B)] for i in range(rows_A)]\n",
    "    \n",
    "    for i in range(rows_A):\n",
    "        for j in range(cols_B):\n",
    "            A_row,B_col = A[i], [x[j] for x in B]\n",
    "            C[i][j] = sum([A_row[k] * B_col[k] for k in range(len_n)])\n",
    "        \n",
    "    return(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e2ab5d-be8f-434e-a602-70f7040d3230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fd62daf-6d87-42b4-b65d-0d26f68f90a6",
   "metadata": {},
   "source": [
    "Now that we have our matrix functions ready, let's define how a neuron calculates a preduction from inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4302c14-9823-498e-93a7-d813729a5c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron(Neuron):\n",
    "\n",
    "    def predict(self, inputs: list[float]):\n",
    "        print(inputs,self.weights)\n",
    "        # make sure we have exactly 1 weight for each input:\n",
    "        assert(len(inputs)==len(self.weights))\n",
    "\n",
    "        weighted_sum = matrix_mult( matrix_transpose([self.weights]) , [inputs]) \n",
    "        biased_weighted_sum = weighted_sum[0][0] + self.bias\n",
    "        \n",
    "        return(self.activation_function(biased_weighted_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef6892a-f4a6-4977-8a9c-21e8b18d4630",
   "metadata": {},
   "source": [
    "At this point, we have an implimentation of an artificial neuron that can be instantiated with \n",
    "random weights and used in computation.  All that's missing is the ability to train the neuron, but before we can think about how to train a neuron, we first need data to train against.  Let's assume we have 2 predictors, x1 and x2, and are trying to predict class Y whose members are Red and Blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4201c86a-9619-4b38-84d3-8c7687639eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pairs of random numbers (x1,x2) between -10 and 10\n",
    "data = [[(random() - .5) * 20, (random() - .5) * 20] for _ in range(10)]\n",
    "\n",
    "# if x1 is greater, assign that data point to class 0, else class 1:\n",
    "for data_point in data:\n",
    "    if data_point[0] >  data_point[1]:\n",
    "        data_point.append(0)\n",
    "    else:\n",
    "        data_point.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bd0acee-8c34-4eca-8e96-e694aa06f716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.913584290546188, 8.063046718945937] [0.7277158617167763, 0.23901038418853748]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.880706594611355"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron = Neuron(2, \"RELU\")\n",
    "neuron.predict(data[1][0:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e49ddd-47a9-4407-952e-41494b6d46bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b263399-9f5d-4b82-9e19-b41a9af442a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_reshape(matrix):\n",
    "    cols = len(matrix)\n",
    "    rows = len(matrix[0])\n",
    "    elements = (elem for row in matrix for elem in row)\n",
    "    mT = []\n",
    "    for i in range(len_x):\n",
    "        mT.append([])\n",
    "        for j in range(len_y):\n",
    "            mT[-1].append(elements.__next__())\n",
    "    return(mT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
